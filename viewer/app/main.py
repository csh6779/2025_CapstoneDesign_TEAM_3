"""
ATI Lab 2025 - Neuroglancer Unified Viewer
JSON-based Authentication (MySQL ì œê±°, ê¸°ì¡´ API í˜•ì‹ ìœ ì§€)
/viewer/app/main.py
"""

import os
import sys
from pathlib import Path
import time
import jwt
import json
import shutil
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from passlib.context import CryptContext

# Python path ì„¤ì •
current_dir = Path(__file__).parent
if str(current_dir) not in sys.path:
    sys.path.insert(0, str(current_dir))

from fastapi import FastAPI, HTTPException, Form, Query, Depends, Request, BackgroundTasks, status
from fastapi.responses import FileResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from starlette.middleware.base import BaseHTTPMiddleware
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from pydantic import BaseModel

# ë¡œê¹… ì„¤ì •
from shared_logging import (
    get_logger,
    set_current_user,
    clear_current_user
)

logger = get_logger("viewer", "/logs")

print("ğŸ”¥ğŸ”¥ğŸ”¥ JSON-based Authentication - ê¸°ì¡´ API í˜•ì‹ ìœ ì§€ ğŸ”¥ğŸ”¥ğŸ”¥")

# ==========================================
# 1. FastAPI ì•± ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
# ==========================================
app = FastAPI(
    title="ATI Lab 2025 Viewer",
    description="Neuroglancer Viewer Backend with JSON-based Auth",
    version="3.0.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==========================================
# 2. ì„¤ì • ë° ê²½ë¡œ
# ==========================================
CONVERTER_UPLOADS = os.getenv("CONVERTER_UPLOADS_DIR", "/mnt/converter_uploads")
F_UPLOADS = os.getenv("F_UPLOADS_DIR", "/mnt/f_uploads")
TMP_UPLOADS = os.getenv("TMP_UPLOADS_DIR", "/mnt/tmp_uploads")
FRONTEND_DIR = "/frontend"
LOG_DIR = "/logs"
DATA_DIR = "/viewer/data"

# ë°ì´í„° ë””ë ‰í„°ë¦¬ ìƒì„±
Path(DATA_DIR).mkdir(parents=True, exist_ok=True)
USERS_FILE = Path(DATA_DIR) / "users.json"
BOOKMARKS_FILE = Path(DATA_DIR) / "bookmarks.json"

# JWT ì„¤ì •
SECRET_KEY = os.getenv("JWT_SECRET_KEY", "your-secret-key-change-this-in-production")
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = int(os.getenv("JWT_ACCESS_TOKEN_EXPIRE_MINUTES", "1440"))

pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")
security = HTTPBearer()

RAW_UPLOAD_DIRS = {}

# ì—…ë¡œë“œ ë””ë ‰í„°ë¦¬ ì„¤ì •
if os.path.exists(CONVERTER_UPLOADS):
    RAW_UPLOAD_DIRS["converter"] = CONVERTER_UPLOADS
if os.path.exists(F_UPLOADS):
    RAW_UPLOAD_DIRS["f_drive"] = F_UPLOADS

# tmp í´ë” ìƒì„± ë° ì¶”ê°€
if not os.path.exists(TMP_UPLOADS):
    try:
        os.makedirs(TMP_UPLOADS, exist_ok=True)
    except Exception as e:
        logger.error(f"Failed to create tmp dir: {e}")
RAW_UPLOAD_DIRS["tmp"] = TMP_UPLOADS

logger.info("=" * 80)
logger.info("ğŸ”¬ ATI Lab 2025 - Neuroglancer Viewer v3.0.0")
logger.info("=" * 80)
logger.info(f"   Converter: {CONVERTER_UPLOADS}")
logger.info(f"   F Drive:   {F_UPLOADS}")
logger.info(f"   Tmp Drive: {TMP_UPLOADS}")
logger.info(f"âœ… Available locations: {list(RAW_UPLOAD_DIRS.keys())}")
logger.info("=" * 80)

# ==========================================
# 3. JSON íŒŒì¼ ê´€ë¦¬ í•¨ìˆ˜
# ==========================================

def load_users() -> Dict:
    """ì‚¬ìš©ì ë°ì´í„° ë¡œë“œ"""
    if not USERS_FILE.exists():
        return {}
    try:
        with open(USERS_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except:
        return {}

def save_users(users: Dict):
    """ì‚¬ìš©ì ë°ì´í„° ì €ì¥"""
    with open(USERS_FILE, 'w', encoding='utf-8') as f:
        json.dump(users, f, indent=2, ensure_ascii=False)

def load_bookmarks() -> Dict:
    """ë¶ë§ˆí¬ ë°ì´í„° ë¡œë“œ"""
    if not BOOKMARKS_FILE.exists():
        return {}
    try:
        with open(BOOKMARKS_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except:
        return {}

def save_bookmarks(bookmarks: Dict):
    """ë¶ë§ˆí¬ ë°ì´í„° ì €ì¥"""
    with open(BOOKMARKS_FILE, 'w', encoding='utf-8') as f:
        json.dump(bookmarks, f, indent=2, ensure_ascii=False)

# ==========================================
# 4. ì¸ì¦ ê´€ë ¨ í•¨ìˆ˜
# ==========================================

def verify_password(plain_password: str, hashed_password: str) -> bool:
    """ë¹„ë°€ë²ˆí˜¸ ê²€ì¦"""
    return pwd_context.verify(plain_password, hashed_password)

def get_password_hash(password: str) -> str:
    """ë¹„ë°€ë²ˆí˜¸ í•´ì‹œ"""
    return pwd_context.hash(password)

def create_access_token(data: dict) -> str:
    """JWT í† í° ìƒì„±"""
    to_encode = data.copy()
    expire = datetime.utcnow() + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    to_encode.update({"exp": expire})
    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)
    return encoded_jwt

def get_current_user_from_token(credentials: HTTPAuthorizationCredentials = Depends(security)) -> Dict:
    """í˜„ì¬ ì‚¬ìš©ì ê°€ì ¸ì˜¤ê¸° (í† í° ê¸°ë°˜)"""
    try:
        token = credentials.credentials
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        login_id = payload.get("LoginId") or payload.get("sub")
        if login_id is None:
            raise HTTPException(status_code=401, detail="Invalid token")
        
        users = load_users()
        if login_id not in users:
            raise HTTPException(status_code=401, detail="User not found")
        
        return users[login_id]
    except jwt.ExpiredSignatureError:
        raise HTTPException(status_code=401, detail="Token expired")
    except jwt.JWTError:
        raise HTTPException(status_code=401, detail="Invalid token")

# ==========================================
# 5. ë¡œê¹… ë¯¸ë“¤ì›¨ì–´
# ==========================================
# 5. ë¡œê¹… ë¯¸ë“¤ì›¨ì–´
class AuthAndLoggingMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        login_id_str = None
        auth_header = request.headers.get("Authorization")
        path = request.url.path

        if auth_header and auth_header.startswith("Bearer "):
            token = auth_header.replace("Bearer ", "")
            try:
                payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
                login_id_str = payload.get("LoginId") or payload.get("sub")
            except:
                pass

        if not login_id_str and path.startswith("/u/"):
            try:
                parts = path.split("/")
                if len(parts) > 2:
                    login_id_str = parts[2]
            except:
                pass

        if not login_id_str:
            login_id_str = request.query_params.get("LoginId")

        # âœ… ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ì„¤ì •
        if login_id_str:
            set_current_user(login_id_str)

        start_time = time.time()
        response = await call_next(request)
        process_time = time.time() - start_time

        if request.method == "GET" and not path.startswith(("/api/health", "/favicon.ico", "/manifest.json", "/static")):
            log_payload = {
                "action": "view_image", "path": path, "method": request.method,
                "status": response.status_code, "duration": round(process_time, 4)
            }
            if response.status_code >= 500:
                logger.error(log_payload)
            elif response.status_code >= 400:
                logger.warning(log_payload)
            else:
                logger.info(log_payload)

        # âœ… ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ í´ë¦¬ì–´
        clear_current_user()
        return response

app.add_middleware(AuthAndLoggingMiddleware)

# ==========================================
# 6. Pydantic ëª¨ë¸
# ==========================================

class LoginRequest(BaseModel):
    username: str
    password: str

class SignupRequest(BaseModel):
    LoginId: str
    UserName: str
    Password: str

class TokenResponse(BaseModel):
    AccessToken: str  # ê¸°ì¡´ í”„ë¡ íŠ¸ì—”ë“œ í˜•ì‹ ìœ ì§€
    LoginId: str
    UserName: str
    Role: str

class UserResponse(BaseModel):
    LoginId: str
    UserName: str
    Role: str

class BookmarkCreate(BaseModel):
    volume_name: str
    location: str
    note: Optional[str] = None

# ==========================================
# 7. ë³¼ë¥¨ ìŠ¤ìº” í•¨ìˆ˜
# ==========================================

def scan_raw_uploads(location: str, calculate_size: bool = False) -> List[Dict]:
    """íŠ¹ì • ìœ„ì¹˜ì˜ precomputed ë°ì´í„°ì…‹ ìŠ¤ìº”"""
    if location not in RAW_UPLOAD_DIRS:
        return []

    base_path = Path(RAW_UPLOAD_DIRS[location])
    if not base_path.exists():
        logger.warning(f"Location path does not exist: {location} -> {base_path}")
        return []

    datasets = []
    try:
        for item in base_path.iterdir():
            if not item.is_dir():
                continue

            info_file = item / "info"
            if not info_file.exists():
                continue

            try:
                with open(info_file, 'r') as f:
                    info_data = json.load(f)

                size_info = {}
                if calculate_size:
                    try:
                        total_size = sum(
                            f.stat().st_size
                            for f in item.rglob('*')
                            if f.is_file()
                        )
                        size_info = {
                            'size_mb': round(total_size / (1024 * 1024), 2),
                            'size_gb': round(total_size / (1024 * 1024 * 1024), 2)
                        }
                    except:
                        size_info = {'size_mb': 0, 'size_gb': 0}

                dataset = {
                    'name': item.name,
                    'location': location,
                    'path': str(item),
                    'type': info_data.get('type', 'image'),
                    'data_type': info_data.get('data_type', 'uint8'),
                    'num_channels': info_data.get('num_channels', 1),
                    'dimensions': info_data['scales'][0]['size'] if 'scales' in info_data else None,
                    'resolution': info_data['scales'][0]['resolution'] if 'scales' in info_data else None,
                    'chunk_sizes': info_data['scales'][0]['chunk_sizes'][0] if 'scales' in info_data else None,
                    'has_info': True,
                    'created_at': datetime.fromtimestamp(item.stat().st_ctime).isoformat(),
                    'modified_at': datetime.fromtimestamp(item.stat().st_mtime).isoformat(),
                    **size_info
                }
                datasets.append(dataset)
            except Exception as e:
                logger.error(f"Error reading info from {item.name}: {e}")
                continue

        logger.info(f"Scanned {location}: found {len(datasets)} precomputed datasets")
        return sorted(datasets, key=lambda x: x['modified_at'], reverse=True)
    except Exception as e:
        logger.error(f"Error scanning {location}: {e}")
        return []

def get_all_datasets() -> Dict[str, List[Dict]]:
    """ëª¨ë“  ìœ„ì¹˜ì˜ ë°ì´í„°ì…‹ ì¡°íšŒ"""
    result = {}
    for location in RAW_UPLOAD_DIRS.keys():
        result[location] = scan_raw_uploads(location)
    return result

# ==========================================
# 8. API ì—”ë“œí¬ì¸íŠ¸
# ==========================================

@app.get("/")
def index():
    """ë©”ì¸ í˜ì´ì§€"""
    index_path = os.path.join(FRONTEND_DIR, "index.html")
    if os.path.exists(index_path):
        return FileResponse(index_path)
    return {"status": "running", "version": "3.0.0", "available_locations": list(RAW_UPLOAD_DIRS.keys())}

@app.get("/api/health")
def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {"status": "healthy", "locations": list(RAW_UPLOAD_DIRS.keys())}

# ==========================================
# ì¸ì¦ API (ê¸°ì¡´ í˜•ì‹ ìœ ì§€)
# ==========================================

@app.post("/api/v1/auth/token", response_model=TokenResponse)
def login(
    username: str = Form(...),
    password: str = Form(...)
):
    """ë¡œê·¸ì¸ - Form ë°ì´í„°ë¡œ ë°›ê¸°"""
    users = load_users()
    
    if username not in users:
        raise HTTPException(status_code=401, detail="Invalid credentials")
    
    user = users[username]
    
    if not verify_password(password, user["PasswordHash"]):
        raise HTTPException(status_code=401, detail="Invalid credentials")
    
    access_token = create_access_token(data={"LoginId": username, "sub": username})
    
    logger.info(f"âœ… User logged in: {username}")
    
    return TokenResponse(
        AccessToken=access_token,  # ê¸°ì¡´ í”„ë¡ íŠ¸ì—”ë“œ í˜•ì‹
        LoginId=user["LoginId"],
        UserName=user["UserName"],
        Role=user["Role"]
    )

@app.post("/api/v1/auth/signup")
def signup(request: SignupRequest):
    """íšŒì›ê°€ì…"""
    users = load_users()
    
    # ì¤‘ë³µ í™•ì¸
    if request.LoginId in users:
        raise HTTPException(status_code=400, detail="User already exists")
    
    # ìƒˆ ì‚¬ìš©ì ìƒì„±
    new_user = {
        "LoginId": request.LoginId,
        "UserName": request.UserName,
        "PasswordHash": get_password_hash(request.Password),
        "Role": "user",
        "CreatedAt": datetime.now().isoformat(),
        "UpdatedAt": datetime.now().isoformat()
    }
    
    users[request.LoginId] = new_user
    save_users(users)
    
    logger.info(f"âœ… New user registered: {request.LoginId}")
    
    return {
        "message": "User created successfully",
        "LoginId": request.LoginId,
        "UserName": request.UserName
    }

@app.get("/api/v1/users/me", response_model=UserResponse)
@app.get("/api/v1/auth/me", response_model=UserResponse)
def get_me(current_user: Dict = Depends(get_current_user_from_token)):
    """í˜„ì¬ ì‚¬ìš©ì ì •ë³´"""
    return UserResponse(
        LoginId=current_user["LoginId"],
        UserName=current_user["UserName"],
        Role=current_user["Role"]
    )

# ==========================================
# ë³¼ë¥¨ API (ê¸°ì¡´ í˜•ì‹ ìœ ì§€)
# ==========================================

@app.get("/api/volumes")
def list_volumes(
    LoginId: Optional[str] = Query(None),
    current_user: Dict = Depends(get_current_user_from_token)
):
    """ë³¼ë¥¨ ëª©ë¡ ì¡°íšŒ"""
    datasets = get_all_datasets()
    volumes_list = []
    
    for loc, items in datasets.items():
        for ds in items:
            volumes_list.append({
                "name": ds["name"],
                "location": loc,
                "dimensions": ds.get("dimensions"),
                "chunk_size": [64, 64, 64],
                "size_gb": round(ds.get("size_mb", 0) / 1024, 2),
            })
    
    logger.info(f"ğŸ“¦ User {current_user['LoginId']} requested volumes: {len(volumes_list)} found")
    
    return {"volumes": volumes_list, "count": len(volumes_list)}

@app.get("/api/admin/volumes")
def list_admin_volumes(current_user: Dict = Depends(get_current_user_from_token)):
    """ê´€ë¦¬ììš© ë³¼ë¥¨ ëª©ë¡"""
    if current_user["Role"] != "admin":
        raise HTTPException(status_code=403, detail="Admin only")
    
    logger.info("Admin volume list requested by admin")
    volume_list = []
    
    for loc_name, loc_path in RAW_UPLOAD_DIRS.items():
        p = Path(loc_path)
        if not p.exists():
            logger.warning(f"âš ï¸ Path not found: {loc_path}")
            continue
        
        try:
            for item in p.iterdir():
                if item.is_dir() and not item.name.startswith(".") and item.name != "temp":
                    volume_list.append({
                        "name": item.name,
                        "location": loc_name
                    })
        except Exception as e:
            logger.error(f"Error scanning {loc_path}: {e}")
    
    logger.info(f"ğŸ” Found volumes: {len(volume_list)} items")
    return volume_list

@app.delete("/api/admin/volumes/{volume_name}")
def delete_volume(
    volume_name: str,
    background_tasks: BackgroundTasks,
    current_user: Dict = Depends(get_current_user_from_token)
):
    """ë³¼ë¥¨ ì‚­ì œ"""
    if current_user["Role"] != "admin":
        raise HTTPException(status_code=403, detail="Admin only")
    
    target_path = None
    found_location = ""
    
    for loc_name, loc_path in RAW_UPLOAD_DIRS.items():
        candidate_path = Path(loc_path) / volume_name
        if candidate_path.exists() and candidate_path.is_dir():
            target_path = candidate_path
            found_location = loc_name
            break
    
    if not target_path:
        raise HTTPException(status_code=404, detail=f"Volume '{volume_name}' not found")
    
    try:
        background_tasks.add_task(shutil.rmtree, target_path)
        logger.info(f"ğŸ—‘ï¸ Admin deleted volume: {volume_name} from {found_location}")
        return {"message": f"Started deletion of '{volume_name}' from {found_location}"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/raw-uploads")
def list_raw_uploads(
    calculate_size: bool = Query(False, description="í´ë” í¬ê¸° ê³„ì‚° ì—¬ë¶€"),
    current_user: Dict = Depends(get_current_user_from_token)
):
    """ë³€í™˜ëœ precomputed ë°ì´í„°ì…‹ ëª©ë¡ ì¡°íšŒ"""
    logger.info(f"User {current_user['LoginId']} listing precomputed datasets")
    
    uploads = {}
    for location in RAW_UPLOAD_DIRS.keys():
        uploads[location] = scan_raw_uploads(location, calculate_size=calculate_size)
    
    total = sum(len(files) for files in uploads.values())
    
    return {
        "uploads": uploads,
        "total": total,
        "locations": list(RAW_UPLOAD_DIRS.keys())
    }

# ==========================================
# ë¶ë§ˆí¬ API
# ==========================================

@app.get("/api/v1/bookmarks")
def list_bookmarks(current_user: Dict = Depends(get_current_user_from_token)):
    """ë¶ë§ˆí¬ ëª©ë¡"""
    bookmarks = load_bookmarks()
    user_bookmarks = bookmarks.get(current_user["LoginId"], [])
    return {"bookmarks": user_bookmarks}

@app.post("/api/v1/bookmarks")
def create_bookmark(
    bookmark: BookmarkCreate,
    current_user: Dict = Depends(get_current_user_from_token)
):
    """ë¶ë§ˆí¬ ìƒì„±"""
    bookmarks = load_bookmarks()
    
    if current_user["LoginId"] not in bookmarks:
        bookmarks[current_user["LoginId"]] = []
    
    new_bookmark = {
        "id": len(bookmarks[current_user["LoginId"]]) + 1,
        "volume_name": bookmark.volume_name,
        "location": bookmark.location,
        "note": bookmark.note,
        "created_at": datetime.now().isoformat()
    }
    
    bookmarks[current_user["LoginId"]].append(new_bookmark)
    save_bookmarks(bookmarks)
    
    logger.info(f"ğŸ“Œ User {current_user['LoginId']} created bookmark: {bookmark.volume_name}")
    return new_bookmark

@app.delete("/api/v1/bookmarks/{bookmark_id}")
def delete_bookmark(
    bookmark_id: int,
    current_user: Dict = Depends(get_current_user_from_token)
):
    """ë¶ë§ˆí¬ ì‚­ì œ"""
    bookmarks = load_bookmarks()
    
    if current_user["LoginId"] not in bookmarks:
        raise HTTPException(status_code=404, detail="No bookmarks found")
    
    user_bookmarks = bookmarks[current_user["LoginId"]]
    bookmarks[current_user["LoginId"]] = [b for b in user_bookmarks if b["id"] != bookmark_id]
    save_bookmarks(bookmarks)
    
    logger.info(f"ğŸ—‘ï¸ User {current_user['LoginId']} deleted bookmark: {bookmark_id}")
    return {"message": "Bookmark deleted"}

# ==========================================
# ë©”ëª¨ë¦¬ ë° ë¡œê·¸ API
# ==========================================

@app.get("/api/v1/memory-status")
def get_memory_status():
    """ë©”ëª¨ë¦¬ ìƒíƒœ ì¡°íšŒ - í”„ë¡ íŠ¸ì—”ë“œ í˜¸í™˜ í˜•ì‹"""
    import psutil
    
    process = psutil.Process()
    memory = psutil.virtual_memory()
    disk = psutil.disk_usage('/')
    
    # í”„ë¡ íŠ¸ì—”ë“œê°€ ê¸°ëŒ€í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
    return {
        "memory": {
            "total": memory.total,
            "available": memory.available,
            "percent": memory.percent,
            "used": memory.used,
            "free": memory.free,
            "process_mb": process.memory_info().rss / (1024 * 1024),
            "system_percent": memory.percent
        },
        "disk": {
            "total": disk.total,
            "used": disk.used,
            "free": disk.free,
            "percent": disk.percent
        },
        "cache": {
            "cache_size_mb": 0,  # JSON ê¸°ë°˜ì—ì„œëŠ” ìº ì‹œ ì—†ìŒ
            "hit_rate": 0.0
        },
        "config": {
            "cache_max_size_mb": 0
        }
    }

@app.post("/api/v1/memory-clean")
def clean_memory(current_user: Dict = Depends(get_current_user_from_token)):
    """ë©”ëª¨ë¦¬ ì •ë¦¬ - ê´€ë¦¬ì ì „ìš©"""
    if current_user["Role"] != "admin":
        raise HTTPException(status_code=403, detail="Admin only")
    
    import gc
    gc.collect()
    
    return {
        "freed_mb": 0,  # ì‹¤ì œë¡œëŠ” ì¸¡ì •í•˜ê¸° ì–´ë ¤ì›€
        "message": "Memory cleanup completed"
    }

# ==========================================
# ì—…ë¡œë“œ API
# ==========================================

from fastapi import File, UploadFile
import aiofiles
from precomputed_writer import convert_image_file_to_precomputed

@app.post("/api/v1/upload")
async def upload_file(
    file: UploadFile = File(...),
    current_user: Dict = Depends(get_current_user_from_token)
):
    """
    íŒŒì¼ ì—…ë¡œë“œ ë° Precomputed í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    ì—…ë¡œë“œëœ íŒŒì¼ì€ TMP_UPLOADS ë””ë ‰í† ë¦¬ì— ë³€í™˜ë˜ì–´ ì €ì¥ë©ë‹ˆë‹¤.
    """
    if current_user["Role"] != "admin":
        raise HTTPException(status_code=403, detail="Admin only")
    
    # íŒŒì¼ í™•ì¥ì í™•ì¸
    allowed_extensions = [".png", ".jpg", ".jpeg", ".tiff", ".tif", ".bmp"]
    file_ext = Path(file.filename).suffix.lower()
    
    if file_ext not in allowed_extensions:
        raise HTTPException(
            status_code=400,
            detail=f"Unsupported file type: {file_ext}. Allowed: {', '.join(allowed_extensions)}"
        )
    
    # ì„ì‹œ ì €ì¥ ê²½ë¡œ
    upload_dir = Path(TMP_UPLOADS)
    upload_dir.mkdir(parents=True, exist_ok=True)
    
    # íŒŒì¼ëª… ì¤‘ë³µ ë°©ì§€ (íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    original_name = Path(file.filename).stem
    safe_filename = f"{timestamp}_{original_name}{file_ext}"
    temp_file_path = upload_dir / safe_filename
    
    # ë³€í™˜ëœ precomputed ë³¼ë¥¨ ì €ì¥ ê²½ë¡œ
    volume_name = f"{timestamp}_{original_name}"
    volume_path = upload_dir / volume_name
    
    try:
        # 1. ì›ë³¸ íŒŒì¼ ì €ì¥
        async with aiofiles.open(temp_file_path, 'wb') as out_file:
            content = await file.read()
            await out_file.write(content)
        
        file_size_mb = len(content) / (1024 * 1024)
        logger.info(f"ğŸ“¤ File uploaded by {current_user['LoginId']}: {safe_filename} ({file_size_mb:.2f} MB)")
        
        # 2. Precomputed í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        logger.info(f"ğŸ”„ Converting {safe_filename} to precomputed format...")
        
        chunk_count = convert_image_file_to_precomputed(
            input_path=str(temp_file_path),
            output_path=str(volume_path),
            chunk_size=512,
            encoding="raw"
        )
        
        logger.info(f"âœ… Conversion completed: {volume_name} ({chunk_count} chunks created)")
        
        # 3. ì›ë³¸ íŒŒì¼ ì‚­ì œ (ì„ íƒì )
        try:
            temp_file_path.unlink()
            logger.info(f"ğŸ—‘ï¸ Temporary file removed: {safe_filename}")
        except:
            pass
        
        return {
            "message": f"File uploaded and converted successfully: {volume_name}",
            "volume_name": volume_name,
            "original_filename": file.filename,
            "size_mb": round(file_size_mb, 2),
            "chunks_created": chunk_count,
            "location": "tmp",
            "neuroglancer_url": f"http://localhost:8080/?json_url=http://localhost:9000/precomp/{volume_name}/info"
        }
        
    except Exception as e:
        logger.error(f"ğŸš¨ Upload/Conversion failed: {e}")
        # ì‹¤íŒ¨ ì‹œ ì„ì‹œ íŒŒì¼ ì •ë¦¬
        try:
            if temp_file_path.exists():
                temp_file_path.unlink()
            if volume_path.exists():
                shutil.rmtree(volume_path)
        except:
            pass
        raise HTTPException(status_code=500, detail=f"Upload/Conversion failed: {str(e)}")

@app.get("/api/logs/files")
def list_log_files():
    """ë¡œê·¸ íŒŒì¼ ëª©ë¡"""
    log_base = Path(LOG_DIR)
    if not log_base.exists():
        return {"logs": {}}
    
    logs = {}
    try:
        for year_dir in sorted(log_base.iterdir(), reverse=True):
            if year_dir.is_dir():
                logs[year_dir.name] = {}
                for month_dir in sorted(year_dir.iterdir(), reverse=True):
                    if month_dir.is_dir():
                        logs[year_dir.name][month_dir.name] = sorted(
                            [f.stem for f in month_dir.glob("*.txt")], 
                            reverse=True
                        )
    except Exception as e:
        logger.error(f"Error listing log files: {e}")
    
    return {"logs": logs}

@app.get("/api/v1/image-logs/me")
def get_my_image_logs(
    skip: int = Query(0, ge=0),
    limit: int = Query(100, ge=1, le=1000),
    start_date: Optional[str] = Query(None),
    end_date: Optional[str] = Query(None),
    level: Optional[str] = Query(None),
    current_user: Dict = Depends(get_current_user_from_token)
):
    """
    í˜„ì¬ ì‚¬ìš©ìì˜ ì´ë¯¸ì§€ ì²˜ë¦¬ ë¡œê·¸ ì¡°íšŒ
    ë¡œê·¸ íŒŒì¼ì—ì„œ í•´ë‹¹ ì‚¬ìš©ìì˜ ë¡œê·¸ë§Œ í•„í„°ë§í•˜ì—¬ ë°˜í™˜
    """
    log_base = Path(LOG_DIR)
    if not log_base.exists():
        return {"logs": [], "total": 0}
    
    login_id = current_user["LoginId"]
    logs = []
    
    try:
        # ë‚ ì§œ ë²”ìœ„ íŒŒì‹±
        start_dt = None
        end_dt = None
        
        if start_date:
            try:
                start_dt = datetime.strptime(start_date, "%Y-%m-%d")
            except:
                pass
        
        if end_date:
            try:
                end_dt = datetime.strptime(end_date, "%Y-%m-%d")
                end_dt = end_dt.replace(hour=23, minute=59, second=59)
            except:
                pass
        
        # ë¡œê·¸ íŒŒì¼ ì½ê¸°
        for year_dir in sorted(log_base.iterdir(), reverse=True):
            if not year_dir.is_dir():
                continue
            
            for month_dir in sorted(year_dir.iterdir(), reverse=True):
                if not month_dir.is_dir():
                    continue
                
                for log_file in sorted(month_dir.glob("*.txt"), reverse=True):
                    try:
                        with open(log_file, 'r', encoding='utf-8') as f:
                            for line in f:
                                line = line.strip()
                                if not line:
                                    continue
                                
                                try:
                                    log_entry = json.loads(line)
                                    
                                    # ì‚¬ìš©ì í•„í„°ë§ - ì—¬ëŸ¬ í•„ë“œ í™•ì¸
                                    log_user = (
                                        log_entry.get("user") or 
                                        log_entry.get("user_id") or 
                                        log_entry.get("LoginId") or 
                                        log_entry.get("login_id")
                                    )
                                    
                                    if log_user != login_id:
                                        continue
                                    
                                    # ë‚ ì§œ í•„í„°ë§
                                    if start_dt or end_dt:
                                        timestamp_str = log_entry.get("timestamp")
                                        if timestamp_str:
                                            try:
                                                log_dt = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
                                                if start_dt and log_dt < start_dt:
                                                    continue
                                                if end_dt and log_dt > end_dt:
                                                    continue
                                            except:
                                                pass
                                    
                                    # ë ˆë²¨ í•„í„°ë§
                                    if level and log_entry.get("level") != level:
                                        continue
                                    
                                    logs.append(log_entry)
                                    
                                except json.JSONDecodeError:
                                    continue
                    except Exception as e:
                        logger.error(f"Error reading log file {log_file}: {e}")
                        continue
        
        # í˜ì´ì§€ë„¤ì´ì…˜
        total = len(logs)
        logs = logs[skip:skip + limit]
        
        return {
            "logs": logs,
            "total": total,
            "skip": skip,
            "limit": limit
        }
        
    except Exception as e:
        logger.error(f"Error fetching image logs: {e}")
        return {"logs": [], "total": 0}

# ==========================================
# Neuroglancer API
# ==========================================

NEUROGLANCER_URL = os.getenv("NEUROGLANCER_URL", "http://localhost:8080")

@app.get("/api/neuroglancer/info")
async def get_neuroglancer_info():
    """Neuroglancer ì„œë²„ ì •ë³´ ì œê³µ"""
    return {
        "url": NEUROGLANCER_URL,
        "local_url": "http://localhost:8080",
        "status": "running",
        "available_locations": list(RAW_UPLOAD_DIRS.keys())
    }

@app.get("/api/neuroglancer/state")
async def create_neuroglancer_state(
    volume_name: str,
    location: str,
    current_user: Dict = Depends(get_current_user_from_token)
):
    """Neuroglancer ìƒíƒœ URL ìƒì„±"""
    if location not in RAW_UPLOAD_DIRS:
        raise HTTPException(status_code=400, detail=f"Invalid location: {location}")

    volume_path = Path(RAW_UPLOAD_DIRS[location]) / volume_name
    if not volume_path.exists():
        raise HTTPException(status_code=404, detail=f"Volume not found: {volume_name}")

    info_file = volume_path / "info"
    if not info_file.exists():
        raise HTTPException(status_code=404, detail="Info file not found")

    with open(info_file, 'r') as f:
        info_data = json.load(f)

    state = {
        "layers": [
            {
                "type": "image",
                "source": f"precomputed://http://localhost:9000/precomp/{volume_name}",
                "name": volume_name,
                "shader": """
void main() {
  emitRGB(vec3(toNormalized(getDataValue(0))));
}
"""
            }
        ],
        "navigation": {
            "pose": {
                "position": {
                    "voxelSize": info_data['scales'][0]['resolution'],
                    "voxelCoordinates": [
                        info_data['scales'][0]['size'][0] // 2,
                        info_data['scales'][0]['size'][1] // 2,
                        info_data['scales'][0]['size'][2] // 2
                    ]
                }
            },
            "zoomFactor": 8
        },
        "layout": "xy-3d"
    }

    from urllib.parse import quote
    state_json = json.dumps(state)
    encoded_state = quote(state_json)

    neuroglancer_url = f"{NEUROGLANCER_URL}/#!{encoded_state}"

    logger.info({
        "action": "create_neuroglancer_state",
        "user": current_user["LoginId"],
        "volume": volume_name,
        "location": location
    })

    return {
        "url": neuroglancer_url,
        "state": state,
        "volume_info": {
            "name": volume_name,
            "location": location,
            "dimensions": info_data['scales'][0]['size'],
            "resolution": info_data['scales'][0]['resolution']
        }
    }

@app.get("/precomp/{volume_name}/{file_path:path}")
async def get_precomputed_file(volume_name: str, file_path: str):
    """Neuroglancer precomputed íŒŒì¼ ì„œë¹™"""
    target_path = None
    
    for loc_name, loc_path in RAW_UPLOAD_DIRS.items():
        candidate_dir = Path(loc_path) / volume_name
        if candidate_dir.exists() and candidate_dir.is_dir():
            target_path = candidate_dir
            break
    
    if not target_path:
        logger.warning(f"âŒ Volume not found for request: {volume_name}")
        raise HTTPException(status_code=404, detail=f"Volume '{volume_name}' not found")
    
    full_file_path = target_path / file_path
    
    if full_file_path.exists() and full_file_path.is_file():
        return FileResponse(full_file_path)
    
    logger.warning(f"âŒ File not found: {full_file_path}")
    raise HTTPException(status_code=404, detail="File not found")

# ==========================================
# Static files
# ==========================================

if os.path.exists(FRONTEND_DIR):
    app.mount("/static", StaticFiles(directory=FRONTEND_DIR), name="static")

for location, path in RAW_UPLOAD_DIRS.items():
    if os.path.exists(path):
        app.mount(f"/precomputed/{location}", StaticFiles(directory=path), name=f"precomputed_{location}")
        logger.info(f"ğŸ“ Mounted: /precomputed/{location} -> {path}")

# ==========================================
# ì‹œì‘ ì´ë²¤íŠ¸
# ==========================================

@app.on_event("startup")
async def startup_event():
    logger.info("ğŸš€ Application Starting...")
    
    # ê¸°ë³¸ ê´€ë¦¬ì ê³„ì • ìƒì„±
    users = load_users()
    if "admin" not in users:
        users["admin"] = {
            "LoginId": "admin",
            "UserName": "Administrator",
            "PasswordHash": get_password_hash("admin1234"),
            "Role": "admin",
            "CreatedAt": datetime.now().isoformat(),
            "UpdatedAt": datetime.now().isoformat()
        }
        save_users(users)
        logger.info("âœ… Default admin account created: admin / admin1234")
    
    # ë¶ë§ˆí¬ íŒŒì¼ ì´ˆê¸°í™”
    if not BOOKMARKS_FILE.exists():
        save_bookmarks({})
        logger.info("âœ… Bookmarks file initialized")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=9000, reload=True)